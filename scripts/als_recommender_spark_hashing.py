# -*- coding: utf-8 -*-
"""Spark ALS Recommender Script (Hashing Trick)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jbVBb8v77jwfWXw71JDSn62FLno_uYE5
"""

# recommender_system_spark.py (Revised v4 - Hashing Trick)
import time
import warnings
import logging
import traceback
from pathlib import Path
import random

# Import Spark specific modules
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
# *** Import HashingTF instead of StringIndexer ***
from pyspark.ml.feature import HashingTF
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
# *** Import IntegerType for UDF ***
from pyspark.sql.types import IntegerType

# --- Configuration ---
# Path where the cleaned Parquet files are located (output from Phase 2)
CLEANED_PATH = "/home/bigdata/amazon_data_cleaned"
# Path pattern for input files
parquet_file_pattern = str(Path(CLEANED_PATH) / "*_cleaned.parquet")
# Output directory for this script's results
OUTPUT_DIR = Path("./spark_recommender_results_hashing") # New output dir for this version
OUTPUT_DIR.mkdir(parents=True, exist_ok=True) # Create directory if it doesn't exist

# Log file configuration
LOG_FILE = OUTPUT_DIR / "recommender_spark_hashing.log"
# ALS Model save path
MODEL_PATH = str(OUTPUT_DIR / "als_model_hashing")
# Results save path
RESULTS_FILE = OUTPUT_DIR / "recommender_results_hashing.txt"

# Spark Configuration (Adjust based on server resources)
# Keep increased driver memory from previous attempt
SPARK_DRIVER_MEMORY = "28g"
SPARK_EXECUTOR_MEMORY = "28g" # Keep same as driver in local mode
SPARK_KRYO_BUFFER_MAX = "512m" # Keep increased Kryo buffer

# ALS / Data Prep Configuration
MIN_USER_REVIEWS = 5 # Drop users with fewer reviews than this
TRAIN_TEST_SPLIT_SEED = 42 # Seed for reproducibility
# *** HashingTF Feature Size for IDs ***
HASHINGTF_NUM_FEATURES_IDS = 2**24 # ~16.7 million features for IDs to reduce collisions
ALS_RANK = 10 # Default rank (latent factors)
ALS_MAX_ITER = 10 # Default max iterations
ALS_REG_PARAM = 0.1 # Default regularization parameter
NUM_RECOMMENDATIONS = 5 # Top N recommendations to generate
NUM_EXAMPLE_USERS = 3 # Number of users to generate recommendations for
# --- End Configuration ---

# --- Setup Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a'), # Log to file (append)
        logging.StreamHandler() # Log to console
    ]
)

logging.info("="*50)
logging.info("Script execution started.")
logging.info("Starting Task 5: Recommender System (ALS) using PySpark (Hashing Trick)...") # Updated log message
script_start_time = time.time()

# --- Initialize Spark Session ---
logging.info("Initializing Spark Session...")
spark = None # Initialize spark variable
try:
    spark = SparkSession.builder \
        .appName("ALSRecommenderHashing") \
        .config("spark.driver.memory", SPARK_DRIVER_MEMORY) \
        .config("spark.executor.memory", SPARK_EXECUTOR_MEMORY) \
        .config("spark.sql.parquet.enableVectorizedReader", "true") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.kryoserializer.buffer.max", SPARK_KRYO_BUFFER_MAX) \
        .master("local[*]") \
        .getOrCreate()

    logging.info(f"Spark Session created. Spark version: {spark.version}")
    logging.info(f"Spark Web UI available at: {spark.sparkContext.uiWebUrl}")
    logging.info(f"Driver Memory: {SPARK_DRIVER_MEMORY}, Executor Memory: {SPARK_EXECUTOR_MEMORY}")
    logging.info(f"Kryo max buffer set to: {SPARK_KRYO_BUFFER_MAX}")

except Exception as e:
    logging.error(f"Fatal Error creating Spark Session: {e}")
    logging.error(traceback.format_exc())
    exit()

# --- Load and Prepare Data ---
df_als = None
try:
    logging.info(f"Loading data from: {parquet_file_pattern}")
    df_load = spark.read.parquet(parquet_file_pattern).select("user_id", "parent_asin", "rating")
    df_load = df_load.dropna(subset=["user_id", "parent_asin", "rating"])
    initial_count = df_load.count()
    logging.info(f"Initial rows with user_id, parent_asin, rating: {initial_count}")
    if initial_count == 0:
         logging.error("No valid rows found after loading and dropping NaNs. Exiting.")
         spark.stop()
         exit()

    # Filter users with fewer than MIN_USER_REVIEWS reviews
    logging.info(f"Filtering users with fewer than {MIN_USER_REVIEWS} reviews...")
    user_counts = df_load.groupBy("user_id").count()
    valid_users = user_counts.filter(F.col("count") >= MIN_USER_REVIEWS)
    df_filtered = df_load.join(F.broadcast(valid_users.select("user_id")), on="user_id", how="inner")
    filtered_count = df_filtered.count()
    logging.info(f"Rows after filtering users: {filtered_count} ({initial_count - filtered_count} rows removed)")

    if filtered_count == 0:
         logging.error("No users remaining after filtering by review count. Exiting.")
         spark.stop()
         exit()

    # --- Convert string IDs to integer IDs using HashingTF ---
    logging.info(f"Converting user_id and parent_asin to integer indices using HashingTF (numFeatures={HASHINGTF_NUM_FEATURES_IDS})...")

    # HashingTF requires input to be an array of strings. Create array columns.
    df_hashed = df_filtered.withColumn("user_id_arr", F.array(F.col("user_id"))) \
                           .withColumn("parent_asin_arr", F.array(F.col("parent_asin")))

    # Create HashingTF transformers
    userHasher = HashingTF(inputCol="user_id_arr", outputCol="userIdVec", numFeatures=HASHINGTF_NUM_FEATURES_IDS)
    itemHasher = HashingTF(inputCol="parent_asin_arr", outputCol="itemIdVec", numFeatures=HASHINGTF_NUM_FEATURES_IDS)

    # Transform the data
    df_hashed = userHasher.transform(df_hashed)
    df_hashed = itemHasher.transform(df_hashed)
    logging.info("Hashing complete. Extracting integer indices...")

    # Define a UDF to extract the single index from the sparse vector output by HashingTF
    # HashingTF on a single element array produces a vector with one non-zero entry.
    extract_hash_index = F.udf(lambda vec: int(vec.indices[0]) if vec.indices.size > 0 else -1, IntegerType())

    # Apply UDF to get integer columns
    df_als = df_hashed.withColumn("userIdInt", extract_hash_index(F.col("userIdVec"))) \
                      .withColumn("itemIdInt", extract_hash_index(F.col("itemIdVec")))

    # Filter out any potential errors from UDF (e.g., if somehow an empty vector was produced)
    df_als = df_als.filter((F.col("userIdInt") >= 0) & (F.col("itemIdInt") >= 0))

    # Convert rating to float
    df_als = df_als.withColumn("rating", F.col("rating").cast("float"))

    # Select only the columns needed for ALS
    df_als = df_als.select("userIdInt", "itemIdInt", "rating")
    logging.info("Data preparation complete (filtering, hashing). Final columns: userIdInt, itemIdInt, rating")
    df_als.printSchema()
    df_als.show(5, truncate=False)

    # Optional: Cache the prepared data if resources allow
    df_als.cache()
    prep_count = df_als.count() # Trigger cache and get final count
    logging.info(f"Prepared data count: {prep_count}. Data cached.")

except Exception as e:
    logging.error(f"Fatal Error during data loading/preparation: {e}")
    logging.error(traceback.format_exc())
    if spark: spark.stop()
    exit()

# --- Train/Test Split ---
trainingData = None
testData = None
try:
    logging.info("Splitting data into training (80%) and test (20%) sets...")
    (trainingData, testData) = df_als.randomSplit([0.8, 0.2], seed=TRAIN_TEST_SPLIT_SEED)

    # Optional: Cache splits if significant reuse (evaluation + recommendations)
    trainingData.cache()
    testData.cache()
    logging.info(f"Training data count: {trainingData.count()}") # Trigger cache
    logging.info(f"Test data count: {testData.count()}") # Trigger cache
    logging.info("Train/Test splits cached.")

except Exception as e:
    logging.error(f"Fatal Error during train/test split: {e}")
    logging.error(traceback.format_exc())
    if spark: spark.stop()
    exit()


# --- ALS Model Training ---
als_model = None
try:
    logging.info("Training ALS model...")
    logging.info(f"Parameters: rank={ALS_RANK}, maxIter={ALS_MAX_ITER}, regParam={ALS_REG_PARAM}")
    start_train = time.time()

    # Use the hashed integer columns
    als = ALS(maxIter=ALS_MAX_ITER, regParam=ALS_REG_PARAM, rank=ALS_RANK,
              userCol="userIdInt", itemCol="itemIdInt", ratingCol="rating",
              coldStartStrategy="drop",
              nonnegative=True)

    als_model = als.fit(trainingData)

    end_train = time.time()
    logging.info(f"ALS model training complete. Time: {end_train - start_train:.2f}s")

    # --- Save the ALS Model ---
    logging.info(f"Saving ALS model to: {MODEL_PATH}")
    als_model.write().overwrite().save(MODEL_PATH)
    logging.info("ALS model saved successfully.")

except Exception as e:
    logging.error(f"Fatal Error during ALS model training or saving: {e}")
    logging.error(traceback.format_exc())
    if spark: spark.stop()
    exit()

# --- Evaluation ---
rmse = -1.0
predictions = None
try:
    logging.info("Evaluating model on test data...")
    start_eval = time.time()

    predictions = als_model.transform(testData)
    logging.info("Generated predictions on test data.")

    evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating",
                                    predictionCol="prediction")
    predictions_cleaned = predictions.na.drop(subset=["prediction"])
    rmse = evaluator.evaluate(predictions_cleaned)

    end_eval = time.time()
    logging.info(f"Evaluation complete. Time: {end_eval - start_eval:.2f}s")
    logging.info(f"Root-mean-square error (RMSE) on test data = {rmse:.4f}")

except Exception as e:
    logging.error(f"Error during model evaluation: {e}")
    logging.error(traceback.format_exc())

# --- Generate Recommendations ---
recommendations_output = [] # Store recommendations for saving later
try:
    logging.info(f"Generating top {NUM_RECOMMENDATIONS} recommendations for {NUM_EXAMPLE_USERS} random test users...")
    test_user_ids = testData.select("userIdInt").distinct()
    num_distinct_test_users = test_user_ids.count()

    if num_distinct_test_users == 0:
        logging.warning("No distinct users found in the test set to generate recommendations for.")
    else:
        num_to_sample = min(NUM_EXAMPLE_USERS, num_distinct_test_users)
        logging.info(f"Sampling {num_to_sample} users from {num_distinct_test_users} distinct test users.")
        random_test_users = test_user_ids.orderBy(F.rand(seed=TRAIN_TEST_SPLIT_SEED)).limit(num_to_sample)
        random_test_users.cache()
        logging.info(f"Selected {random_test_users.count()} users for recommendation demo.")

        userRecs = als_model.recommendForUserSubset(random_test_users, NUM_RECOMMENDATIONS)
        logging.info("Generated recommendations.")

        userRecs_list = userRecs.collect()
        recommendations_output.append("\n--- Sample Recommendations ---")
        recommendations_output.append(f"(Note: User/Item IDs are hashed integer representations, not original strings)")
        for row in userRecs_list:
            user_id = row['userIdInt']
            recs = row['recommendations']
            # Note: recs contain 'itemIdInt' (hashed) and 'rating' (predicted)
            rec_string = ", ".join([f"(Item: {r['itemIdInt']}, Rating: {r['rating']:.4f})" for r in recs])
            log_msg = f"User {user_id}: {rec_string}"
            logging.info(log_msg)
            recommendations_output.append(log_msg)

        random_test_users.unpersist()

except Exception as e:
    logging.error(f"Error during recommendation generation: {e}")
    logging.error(traceback.format_exc())

# --- Save Results to File ---
try:
    logging.info(f"Saving RMSE and recommendations to: {RESULTS_FILE}")
    with open(RESULTS_FILE, 'w') as f: # Overwrite results file each time
        f.write("ALS Recommender System Results (using HashingTF for IDs)\n")
        f.write("---------------------------------------------------------\n")
        f.write(f"Root-mean-square error (RMSE) on test data = {rmse:.4f}\n")
        if recommendations_output:
            for line in recommendations_output:
                f.write(line + "\n")
        else:
            f.write("\nNo recommendations were generated.\n")
    logging.info("Results saved successfully.")
except Exception as e:
     logging.error(f"Error saving results to file: {e}")
     logging.error(traceback.format_exc())


# --- Cleanup ---
finally:
    logging.info("Cleaning up cached dataframes...")
    # Unpersist cached dataframes if they exist
    if df_als: df_als.unpersist()
    if trainingData: trainingData.unpersist()
    if testData: testData.unpersist()
    if predictions: predictions.unpersist() # Unpersist predictions df if created
    logging.info("Unpersisted cached data.")

    logging.info("Stopping Spark Session...")
    if spark: spark.stop()
    logging.info("Spark Session stopped.")

script_end_time = time.time()
total_time = script_end_time - script_start_time
logging.info("--- Recommender System (Task 5 using Spark ALS - Hashing) Completed ---")
logging.info(f"Total script time taken: {total_time:.2f} seconds ({total_time / 3600:.2f} hours)")
print(f"\n--- Recommender System (Task 5 using Spark ALS - Hashing) Completed ---") # Final confirmation
print(f"Total time taken: {total_time:.2f} seconds")
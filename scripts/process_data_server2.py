# -*- coding: utf-8 -*-
"""process_data_server.py (Batch Processing)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FFcX5l1Rt_HSPB_MBGvfZfs1MlkFxmDk
"""

# process_data_server.py
import pandas as pd
from pathlib import Path
import time
# Import specific types for better type checking
from datasets import load_from_disk, Dataset, DatasetDict
import gc # Import garbage collector
import pyarrow as pa
import pyarrow.parquet as pq

# Import VALID_CATEGORIES from the utility script
try:
    from bigdata_a3_utils import VALID_CATEGORIES
except ImportError:
    print("Error: Could not import VALID_CATEGORIES from bigdata_a3_utils.py.")
    VALID_CATEGORIES = [ # Manual fallback list
        "All_Beauty", "Amazon_Fashion", "Appliances", "Arts_Crafts_and_Sewing", "Automotive",
        "Baby_Products", "Beauty_and_Personal_Care", "Books", "CDs_and_Vinyl",
        "Cell_Phones_and_Accessories", "Clothing_Shoes_and_Jewelry", "Digital_Music", "Electronics",
        "Gift_Cards", "Grocery_and_Gourmet_Food", "Handmade_Products", "Health_and_Household",
        "Health_and_Personal_Care", "Home_and_Kitchen", "Industrial_and_Scientific", "Kindle_Store",
        "Magazine_Subscriptions", "Movies_and_TV", "Musical_Instruments", "Office_Products",
        "Patio_Lawn_and_Garden", "Pet_Supplies", "Software", "Sports_and_Outdoors",
        "Subscription_Boxes", "Tools_and_Home_Improvement", "Toys_and_Games", "Video_Games", "Unknown"
    ]

# --- Configuration ---
DATA_PATH = Path("/home/bigdata/amazon_data_uncompressed")
CLEANED_PATH = Path("/home/bigdata/amazon_data_cleaned")
CLEANED_PATH.mkdir(parents=True, exist_ok=True)
# *** Define Batch Size for processing large review datasets ***
# Adjust based on available memory. Start with ~500k or 1M. Lower if OOM persists.
BATCH_SIZE = 1000000
# --- End Configuration ---

# --- Define categories to process ---
categories_to_process = VALID_CATEGORIES
# --- End Categories Definition ---

# --- Define Columns Needed ---
review_cols_needed = [
    'parent_asin', 'rating', 'text', 'user_id', 'asin', 'timestamp',
    'verified_purchase', 'helpful_vote'
]
meta_cols_needed = [
    'parent_asin', 'main_category', 'brand'
]
if 'parent_asin' not in review_cols_needed: review_cols_needed.append('parent_asin')
if 'parent_asin' not in meta_cols_needed: meta_cols_needed.append('parent_asin')
# --- End Columns Needed ---

start_time_total = time.time()
print(f"Starting processing from: {DATA_PATH}")
print(f"Saving cleaned data to: {CLEANED_PATH}")
print(f"Using batch size: {BATCH_SIZE}")

for category in categories_to_process:
    print(f"\n--- Processing Category: {category} ---")
    start_time_category = time.time()

    # Check if already processed
    output_file = CLEANED_PATH / f"{category}_cleaned.parquet"
    if output_file.exists():
        print(f"Output file already exists: {output_file}. Skipping category.")
        continue

    review_folder = DATA_PATH / f"raw_review_{category}"
    meta_folder = DATA_PATH / f"raw_meta_{category}"

    if not review_folder.is_dir():
        print(f"Review folder not found: {review_folder}. Skipping category.")
        continue
    if not meta_folder.is_dir():
        print(f"Metadata folder not found: {meta_folder}. Skipping category.")
        continue

    # Initialize parquet writer and schema variables
    parquet_writer = None
    arrow_schema = None

    try:
        # --- Load FULL metadata (usually smaller) ---
        print(f"Loading metadata for {category} from {meta_folder}...")
        meta_ds_loaded = load_from_disk(str(meta_folder))
        if isinstance(meta_ds_loaded, (dict, DatasetDict)):
            if not meta_ds_loaded: raise ValueError(f"Meta dict empty for {category}.")
            meta_split_name = list(meta_ds_loaded.keys())[0]
            print(f"Using metadata split: '{meta_split_name}'")
            meta_ds = meta_ds_loaded[meta_split_name]
        elif isinstance(meta_ds_loaded, Dataset):
            meta_ds = meta_ds_loaded
        else: raise TypeError(f"Unexpected meta type for {category}: {type(meta_ds_loaded)}")

        actual_meta_cols = [col for col in meta_cols_needed if col in meta_ds.column_names]
        print(f"Selecting metadata columns: {actual_meta_cols}")
        meta_df = meta_ds.select_columns(actual_meta_cols).to_pandas()
        print(f"Metadata DataFrame loaded with shape: {meta_df.shape}")
        del meta_ds_loaded, meta_ds # Free memory
        gc.collect()

        # --- Load and Process Reviews in Batches ---
        print(f"Loading review data for {category} from {review_folder}...")
        review_ds_loaded = load_from_disk(str(review_folder))
        if isinstance(review_ds_loaded, (dict, DatasetDict)):
             if not review_ds_loaded: raise ValueError(f"Review dict empty for {category}.")
             review_split_name = list(review_ds_loaded.keys())[0]
             print(f"Using review split: '{review_split_name}'")
             review_ds = review_ds_loaded[review_split_name]
        elif isinstance(review_ds_loaded, Dataset):
             review_ds = review_ds_loaded
        else: raise TypeError(f"Unexpected review type for {category}: {type(review_ds_loaded)}")

        # Select columns *before* iterating
        actual_review_cols = [col for col in review_cols_needed if col in review_ds.column_names]
        print(f"Selecting review columns for iteration: {actual_review_cols}")
        review_ds_selected = review_ds.select_columns(actual_review_cols)

        total_rows_processed = 0
        batch_num = 0

        # Iterate through the review dataset in batches
        for review_batch in review_ds_selected.iter(batch_size=BATCH_SIZE):
            batch_num += 1
            print(f"\nProcessing Batch {batch_num}...")
            # Convert batch (dict) to pandas DataFrame
            review_batch_df = pd.DataFrame(review_batch)
            print(f"Batch shape: {review_batch_df.shape}")

            # --- Merge current batch with full metadata ---
            if 'parent_asin' not in review_batch_df.columns or 'parent_asin' not in meta_df.columns:
                 print(f"Warning: 'parent_asin' column missing in review batch or meta. Skipping merge.")
                 merged_batch_df = review_batch_df
                 if 'brand' not in merged_batch_df.columns: merged_batch_df['brand'] = "Unknown"
                 if 'main_category' not in merged_batch_df.columns: merged_batch_df['main_category'] = "Unknown"
            else:
                 merged_batch_df = pd.merge(review_batch_df, meta_df, on='parent_asin', how='left')

            # --- Apply Cleaning Steps to the Batch ---
            # (Same cleaning logic as before, but applied to merged_batch_df)
            merged_batch_df.dropna(subset=['rating'], inplace=True)
            merged_batch_df = merged_batch_df[merged_batch_df['rating'].between(1, 5)]
            merged_batch_df.dropna(subset=['text'], inplace=True)
            merged_batch_df = merged_batch_df[merged_batch_df['text'].str.strip() != '']

            if 'brand' not in merged_batch_df.columns: merged_batch_df['brand'] = "Unknown"
            else: merged_batch_df['brand'] = merged_batch_df['brand'].fillna('Unknown').replace('', 'Unknown')

            if 'main_category' not in merged_batch_df.columns: merged_batch_df['main_category'] = "Unknown"
            else: merged_batch_df['main_category'] = merged_batch_df['main_category'].fillna('Unknown').replace('', 'Unknown')

            dup_cols = ['user_id', 'asin', 'text']
            if all(col in merged_batch_df.columns for col in dup_cols):
                 merged_batch_df.drop_duplicates(subset=dup_cols, keep='first', inplace=True)
            # else: print(f"Warning: Cannot drop duplicates for batch {batch_num} - missing columns") # Less verbose

            # --- Derived Columns for the Batch ---
            if 'text' in merged_batch_df.columns:
                 merged_batch_df['review_length'] = merged_batch_df['text'].fillna('').str.split().str.len()
            else: merged_batch_df['review_length'] = 0

            if 'timestamp' in merged_batch_df.columns:
                 merged_batch_df['timestamp_dt'] = pd.to_datetime(merged_batch_df['timestamp'], unit='s', errors='coerce')
                 merged_batch_df['year'] = merged_batch_df['timestamp_dt'].dt.year
                 merged_batch_df.drop(columns=['timestamp_dt'], inplace=True) # Drop intermediate
            else: merged_batch_df['year'] = pd.NA

            # --- Select Final Columns for the Batch ---
            output_columns = [
                'user_id', 'asin', 'parent_asin', 'rating', 'text',
                'review_length', 'year', 'main_category', 'brand',
                'verified_purchase', 'helpful_vote'
            ]
            final_columns = [col for col in output_columns if col in merged_batch_df.columns]
            final_batch_df = merged_batch_df[final_columns].copy()

            # --- Append Batch to Parquet File ---
            # Convert cleaned batch DataFrame to Arrow Table
            arrow_table = pa.Table.from_pandas(final_batch_df, preserve_index=False)

            if parquet_writer is None:
                # For the first batch, create the writer and store the schema
                print(f"Creating Parquet file: {output_file}")
                arrow_schema = arrow_table.schema
                parquet_writer = pq.ParquetWriter(output_file, arrow_schema)

            # Ensure schema consistency (important if types change unexpectedly)
            if arrow_table.schema != arrow_schema:
                 print(f"Warning: Schema mismatch in batch {batch_num}. Attempting to cast.")
                 # Handle schema mismatch if necessary, e.g., casting columns
                 # This might indicate upstream data issues
                 arrow_table = arrow_table.cast(arrow_schema)


            # Write the current batch (as Arrow Table) to the Parquet file
            parquet_writer.write_table(arrow_table)
            total_rows_processed += len(final_batch_df)
            print(f"Batch {batch_num} written. Total rows processed for {category}: {total_rows_processed}")

            # Clean up batch dataframes
            del review_batch_df, merged_batch_df, final_batch_df, arrow_table
            gc.collect()

        # --- Finished Processing Category ---
        # Clean up review dataset object
        del review_ds_loaded, review_ds, review_ds_selected
        gc.collect()

    except Exception as e:
        print(f"Error processing category {category}: An unexpected error occurred - {type(e).__name__}: {e}")
        # import traceback
        # print(traceback.format_exc())
    finally:
        # --- Close Parquet Writer ---
        if parquet_writer is not None:
            print(f"Closing Parquet file for {category}.")
            parquet_writer.close()
            print(f"Category {category} processing finished.")
        # Ensure metadata DataFrame is deleted even if errors occurred mid-batch
        if 'meta_df' in locals():
            del meta_df
            gc.collect()


    end_time_category = time.time()
    print(f"Time taken for {category}: {end_time_category - start_time_category:.2f} seconds")
    print("-" * 50)

# --- Overall Timing ---
end_time_total = time.time()
print(f"\n--- Total Processing Time: {end_time_total - start_time_total:.2f} seconds ---")

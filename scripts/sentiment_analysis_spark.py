# -*- coding: utf-8 -*-
"""Revised Spark Sentiment Analysis Script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WRKAr50v755xfUpCCVQrBLSTjVUyUVhE
"""

# sentiment_analysis_spark.py (Revised)
import time
import warnings
import logging
import traceback
from pathlib import Path

# Import Spark specific modules
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics # For confusion matrix helper

# Import plotting modules (ensure installed: pip install matplotlib scikit-learn)
import matplotlib
matplotlib.use('Agg') # Use Agg backend for non-interactive saving of plots
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay


# --- Configuration ---
# Path where the cleaned Parquet files are located
CLEANED_PATH = "/home/bigdata/amazon_data_cleaned"
# Path pattern for input files
parquet_file_pattern = str(Path(CLEANED_PATH) / "*_cleaned.parquet")
# Output directory for this script's results
OUTPUT_DIR = Path("./spark_sentiment_results") # Save results in a dedicated subdir
OUTPUT_DIR.mkdir(parents=True, exist_ok=True) # Create directory if it doesn't exist

# Log file configuration
LOG_FILE = OUTPUT_DIR / "sentiment_analysis_spark.log"
# Model save path
MODEL_PATH = str(OUTPUT_DIR / "sentiment_spark_pipeline_model")
# Plot save path
PLOT_PATH = OUTPUT_DIR / "sentiment_confusion_matrix_spark.png"
# Metrics save path
METRICS_FILE = OUTPUT_DIR / "evaluation_metrics.txt"


# Spark Configuration (Adjust based on server resources)
SPARK_DRIVER_MEMORY = "20g" # e.g., "16g", "32g". Leave RAM for OS.
SPARK_EXECUTOR_MEMORY = "20g" # If running locally, driver/executor share memory

# ML Configuration
HASHINGTF_NUM_FEATURES = 2**18 # ~262k features
LOGISTIC_REGRESSION_MAXITER = 80 # Max iterations for LR training
TRAIN_TEST_SPLIT_SEED = 37 # Seed for reproducibility
# --- End Configuration ---

# --- Setup Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='w'), # Log to file (overwrite)
        logging.StreamHandler() # Log to console
    ]
)

# Ignore specific warnings if needed (optional)
warnings.filterwarnings("ignore", category=FutureWarning)
logging.info("Script execution started.")
logging.info("Starting Task 4: Sentiment Analysis using PySpark...")
script_start_time = time.time()

# --- Initialize Spark Session ---
logging.info("Initializing Spark Session...")
spark = None # Initialize spark variable
try:
    spark = SparkSession.builder \
        .appName("SentimentAnalysis") \
        .config("spark.driver.memory", SPARK_DRIVER_MEMORY) \
        .config("spark.executor.memory", SPARK_EXECUTOR_MEMORY) \
        .config("spark.sql.parquet.enableVectorizedReader", "true") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .master("local[*]") \
        .getOrCreate() # Use all available cores locally

    logging.info(f"Spark Session created. Spark version: {spark.version}")
    logging.info(f"Spark Web UI available at: {spark.sparkContext.uiWebUrl}")

except Exception as e:
    logging.error(f"Fatal Error creating Spark Session: {e}")
    logging.error(traceback.format_exc())
    exit()

# --- Load Data ---
logging.info(f"Loading data from: {parquet_file_pattern}")
df_spark = None # Initialize df variable
try:
    # Load only rating and text columns initially
    df_spark = spark.read.parquet(parquet_file_pattern).select("rating", "text")
    # Drop rows with nulls in essential columns
    df_spark = df_spark.dropna(subset=["rating", "text"])

    # Count rows (triggers computation)
    row_count = df_spark.count()
    logging.info(f"Data loaded successfully. Total rows after dropna: {row_count}")
    if row_count == 0:
        logging.error("Error: No data loaded after filtering NaNs. Exiting.")
        spark.stop()
        exit()

except Exception as e:
    logging.error(f"Fatal Error loading data with Spark: {e}")
    logging.error(traceback.format_exc())
    if spark: spark.stop()
    exit()

# --- Preprocessing & Feature Engineering Pipeline ---
logging.info("--- Setting up Spark ML Pipeline ---")
pipelineModel = None # Initialize variable
trainingData = None
testData = None
try:
    # 1. Create numerical sentiment label (Spark ML prefers doubles)
    #    Positive (rating > 3) -> 1.0
    #    Negative (rating <= 3) -> 0.0
    df_spark = df_spark.withColumn(
        "label",
        F.when(F.col("rating") > 3, 1.0).otherwise(0.0)
    )
    logging.info("Created 'label' column (0.0 for rating <= 3, 1.0 for rating > 3).")

    # Define Pipeline Stages
    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
    hashingTF = HashingTF(inputCol="filtered_words", outputCol="rawFeatures", numFeatures=HASHINGTF_NUM_FEATURES)
    idf = IDF(inputCol="rawFeatures", outputCol="features") # 'features' is conventional name
    lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=LOGISTIC_REGRESSION_MAXITER)

    # Build the Pipeline
    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])
    logging.info(f"Pipeline created with {len(pipeline.getStages())} stages.")
    logging.info(f"HashingTF numFeatures={HASHINGTF_NUM_FEATURES}, LogisticRegression maxIter={LOGISTIC_REGRESSION_MAXITER}")

    # --- Train/Test Split ---
    logging.info("--- Splitting Data (80/20) ---")
    (trainingData, testData) = df_spark.randomSplit([0.8, 0.2], seed=TRAIN_TEST_SPLIT_SEED)

    # Cache data splits for potentially faster training/evaluation if memory/disk allows
    # Caching triggers computation, so count is efficient here.
    # Note: Caching effectiveness depends heavily on available resources.
    trainingData.cache()
    testData.cache()

    train_count = trainingData.count()
    test_count = testData.count()
    logging.info(f"Training data count: {train_count}")
    logging.info(f"Test data count: {test_count}")
    logging.info("Training and Test data cached (if resources permit).")

    if train_count == 0 or test_count == 0:
         logging.error("Error: Training or test data split resulted in zero rows. Exiting.")
         if spark: spark.stop()
         exit()

    # --- Train the Model ---
    logging.info("--- Task 4c: Training Logistic Regression model via Pipeline ---")
    start_train = time.time()

    # Fit the entire pipeline to the training data
    pipelineModel = pipeline.fit(trainingData)

    end_train = time.time()
    logging.info(f"Model training complete. Time: {end_train - start_train:.2f}s")

    # --- Save the Pipeline Model ---
    # Overwrite allows rerunning without manually deleting the old model
    logging.info(f"Saving pipeline model to: {MODEL_PATH}")
    pipelineModel.write().overwrite().save(MODEL_PATH)
    logging.info(f"Pipeline model saved successfully.")

except Exception as e:
    logging.error(f"An error occurred during pipeline setup or model training: {type(e).__name__}: {e}")
    logging.error(traceback.format_exc())
    if spark: spark.stop()
    exit()

# --- Evaluation ---
logging.info("--- Task 4d: Evaluating model on the test set ---")
start_eval = time.time()
accuracy = -1.0
f1_weighted = -1.0
cm = None
results_df = None

try:
    # Make predictions on the test data
    predictions = pipelineModel.transform(testData)
    logging.info("Predictions generated on test data.")

    # Select columns needed for evaluation and cache for multiple evaluations
    results_df = predictions.select("prediction", "label").cache()
    results_count = results_df.count() # Trigger caching and check count
    logging.info(f"Results DataFrame count: {results_count}")

    # --- Accuracy ---
    evaluator_acc = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator_acc.evaluate(results_df)
    logging.info(f"Accuracy: {accuracy:.4f}")

    # --- F1 Score ---
    evaluator_f1 = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
    f1_weighted = evaluator_f1.evaluate(results_df)
    logging.info(f"F1 Score (Weighted): {f1_weighted:.4f}")

    # --- Confusion Matrix ---
    logging.info("Calculating Confusion Matrix...")
    # Convert to RDD for MulticlassMetrics
    predictionAndLabels = results_df.rdd.map(lambda p: (float(p.prediction), float(p.label)))
    metrics = MulticlassMetrics(predictionAndLabels)
    cm = metrics.confusionMatrix().toArray() # Get confusion matrix as numpy array

    logging.info("Confusion Matrix:")
    logging.info("Labels: [Negative(0.0), Positive(1.0)]")
    logging.info(f"\n{cm}") # Log the matrix array

    # --- Plot Confusion Matrix ---
    logging.info("Attempting to plot confusion matrix...")
    try:
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
        fig, ax = plt.subplots(figsize=(6, 6))
        disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d') # Add values_format
        plt.title('Confusion Matrix for Sentiment Prediction (Spark)')
        plt.tight_layout() # Adjust layout
        plt.savefig(PLOT_PATH)
        logging.info(f"Confusion matrix plot saved to: {PLOT_PATH}")
        plt.close(fig) # Close the figure to free memory
    except Exception as plot_e:
        logging.warning(f"Could not generate confusion matrix plot (requires matplotlib & scikit-learn): {plot_e}")

    # --- Save Metrics to File ---
    logging.info(f"Saving evaluation metrics to: {METRICS_FILE}")
    with open(METRICS_FILE, 'w') as f:
        f.write("Evaluation Metrics:\n")
        f.write("--------------------\n")
        f.write(f"Accuracy: {accuracy:.4f}\n")
        f.write(f"F1 Score (Weighted): {f1_weighted:.4f}\n\n")
        f.write("Confusion Matrix:\n")
        f.write("Labels: [Negative(0.0), Positive(1.0)]\n")
        f.write(f"{cm}\n")
    logging.info("Metrics saved successfully.")

    end_eval = time.time()
    logging.info(f"Evaluation complete. Time: {end_eval - start_eval:.2f}s")
    logging.info("Interpretation: [Discuss model performance based on Spark metrics... Accuracy, Weighted F1. Analyze confusion matrix.]")

except Exception as e:
    logging.error(f"An error occurred during evaluation: {type(e).__name__}: {e}")
    logging.error(traceback.format_exc())

finally:
    # --- Cleanup ---
    # Unpersist cached dataframes if they exist
    if trainingData: trainingData.unpersist()
    if testData: testData.unpersist()
    if results_df: results_df.unpersist()
    logging.info("Unpersisted cached data.")

    logging.info("Stopping Spark Session...")
    if spark: spark.stop()
    logging.info("Spark Session stopped.")

script_end_time = time.time()
total_time = script_end_time - script_start_time
logging.info("--- Sentiment Analysis (Task 4 using Spark) Completed ---")
logging.info(f"Total script time taken: {total_time:.2f} seconds ({total_time / 3600:.2f} hours)")
print(f"\n--- Sentiment Analysis (Task 4 using Spark) Completed ---") # Also print final confirmation
print(f"Total time taken: {total_time:.2f} seconds")